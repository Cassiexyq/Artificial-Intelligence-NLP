> **word2Vec中size参数的选择**(引自Mr.Gao)
>
> 维度的确定并不是有一个固定的值，主要的衡量指标是，如果我们知道这个里边的单词数目比较多，那么维度就相应要大一些，例如 google训练的向量，就需要 500 维度。 如果是自己的小文本，就可以是几十维度。
>
> 1. 如果训练出来的单词，我们获取的 most_similar 和这个单词几乎没什么关系，说明我们的维度设置的太大了，因为维度太大，数据量还不足够让我们的向量“调整”到我们需要的空间位置。
> 2. 如果训练出来的单词，我们的获得的 most_similar 和这个单词有关系，但是获得的 most_similar 单词都是这个单词前后左右的，例如输入“美丽”，本来应该是出现“漂亮”，但是输出的是“的人”。 这个时候很可能是我们的维度太小了，因为我们的数据太多，维度太小，没有把不同的单词拉开空间距离的差距。

#### Q：gensim的word2vec模型训练完后本身就带有了most_similar的方法去寻找某个词语的相似词语。为何还需要get_related_words（week6中）这个函数，使用most_similar方法去层层扩展相似词语的相似词语。

> A：most_similar 是通过机器学习方式获得的词向量以及词向量之间的距离衡量的“相似”词汇。 但是这个相似词汇首先是从**直觉**上来讲，一个单词只有5 - 10个单词和这个单词的意思解决； 另外，从理论上来讲，我们词向量学到的词汇，most_similar 其实是**位置**相同的单词，但是位置相同只能加强**意思**相同。这两者之间是加强关系，不是充分必要的关系。 所以，我们例如 most_similar 能够获得 5 - 10 个同义词的这个现象，结合图搜索算法，获得了更多的同义词 

####  Word2Vec 和WordEmbedding的区别

> A:  一个是概念，一个是实现手段，word embedding就是词向量是一个概念，为区分one-hot的词向量，又叫词嵌入，实现词向量的具体方式有word2vec方式
>
> NLP 词的表示方法类型有两种
>
> * onehot独热编码：是最常用的词表示方法，这种方法把每个词表示为一个很长的向量，向量的维度是词表大小，绝大多数元素为0，只有一个维度为1，这个维度就表示当前的词
>
> **1、向量的维度会随着句子的词的数量类型增大而增大，这个维度等于词典的大小，在动辄上万甚至百万的词典面临巨大的维度灾难问题；**
>
> **2、任意两个词之间都是孤立的，根本无法表示出在语义层面上词语词之间的相关信息，而这一点是致命的**
>
> * 词的分布式表示：传统的独热编码仅仅是将词符号化，不包含任何语义信息，如何将语义融入到词中，能否用一个连续的稠密向量取刻画一个word的特征，这样不仅可以刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上，这个稠密向量被称为**distributed representation**，

#### Q：文本向量化中，得到了一个词典，这里是每个文本都有同样的词典，所以会有行上大多为空的情况，这是独热编码吗？

> 

#### Q：在发生过拟合时我们会添加正则或增加正则，那为什么不一开始就加正则？

>A:  因为如果一开始加入正则，会使损失函数目标变成降尽量选取正则后的随机数取小，而不是降低预测值和真实值之间的bias。一般，我们要将模型调到差不多过拟合，然后加正则。

#### Q：欠拟合和过拟合

> 欠拟合：模型没有很好捕捉到数据特征，不能很好拟合数据
>
> 解决办法：
>
> 1. 添加其他特征项，有时候出现欠拟合是因为特征项不够导致，（组合，泛化，相关性）
>
>   		2. 添加多项式特征，模型泛化能力更强；
>   		3. 减少正则化参数
>
> 过拟合：模型把数据学习的太彻底，以至于把噪声特征也学习了，泛化能力太差
>
> 解决办法：
>
> 1. 重新清洗数据，可能数据不纯；
> 2. 增大数据训练量，可能训练量太小，训练数据占总数据比例小
> 3. 采用正则化（机器学习），L0 L1 L2，目标函数后加上对应范数，一般使用L2； 
> 4. dropout方法（神 经网络）

#### Q：交叉熵损失函数

>预测输出表征成当前标签为1的概率   $$ \hat{y} = P(y = 1| x) $$
>
>样本标签为0的概率 $$1-\hat{y} = p(y=0|x)$$
>
>从极大似然角度出发将两个整合到一起： $$p(y|x) = \hat{y}^{y} \cdot (1-\hat{y})^{1-y}  $$
>
>加入log函数，因为log运算不影响函数本身的单调性，
>
>$$ logP(y|x) = log(\hat{y}^{y} \cdot (1-\hat{y}^{1-y})) = ylog\hat {y} + (1-y)log(1-\hat{y}) $$
>
>希望p(y|x)概率越大越好，反过来，只要log(y|x)的负值越小就行，引入损失函数，令loss = -log(y|x)，即损失函数为：
>
>$$ L =  - [y log \hat{y} + (1-y)log (1- \hat{y}) ]$$
>
>多样本的损失函数，就是每个样本的叠加的平均值
>
>$$ L=-\frac{1}{N} \sum_{i=1}^{N} [y^{i} + (1-y^i)log(1-\hat{y}^i) ]$$
>
>**多样本**：就是cross-entropy的平均值， 例如最后一层求出来是: [-192, 345, 980, 123] softmax之后是: [0.1, 0.3, 0.4, 0.2] 正确答案的label是 [0, 1, 1, 0] 那么loss就是 - 1/4 * ( 0 * log(0.1) + 1 * log(0.3) + 1 * log(0.4) + 0 * log(0.2) )  

#### Q：为什么要使用logistic函数，与感知机，MSE相比的优点

> <https://tech.meituan.com/2015/05/08/intro-to-logistic-regression.html>
>
> 一开始我们使用感知机，感知机是将所有特征与权重做点击，与阈值比较，使用logistic函数，对比感知机，1.输入范围从无穷小到无穷大，y范围在（0，1），满足概率分布。
>
> 2.用概率描述分类器，比用阈值好
>
> 3.单调上升，具有良好的连续性，不存在不连续点。
>
> MSE其实也可以用到逻辑回归。例如我们也可以写成$(\hat{y}-y)^2$但是这样的问题是，如果$\hat{y}$是0，那么loss就是 $\hat{y}^2$, 如果y是1，那么loss也是 $(\hat{y})^2-2\hat{y}+1​$， 这两个比起对数的问题，就是对数更能从低概率快速变化到概率，例如说，log(0.2)的时候，梯度就很多，这样的话，相等于这个loss就更能让我们的预测值快速到1. 

#### Q：sigmod 和softmax

>A：softmax，称为归一化指数函数，能将一个含任意实数的K维向量压缩到另一个k维实向量中，使得每一个元素的范围都在（0，1）之间，并且所有元素的和为1，是二分类任务的推广，用于解决多分类问题，输出值是一组小数，有几类就有几个小数，相加为1
>
>$$Softmax：S(x) = \frac{e^{x_j}}{\sum_{k=1}^k e^{x_k}}, j=1,2,\dots,k $$
>
>Sigmod，作为输出层函数解决二分类问题，输出值是一个小数，除此之外用作隐层的激活函数，激活函数是解释神经网络非线性的核心问题
>
>$$Sigmod : S(x) = \frac{1}{1+e^{-x}}$$

#### Q：交叉熵，相对熵，信息熵，KL散度

> A：在机器学习中，比如分类问题，如果把结果当作是概率分布来看，标签表示的就是数据真实的概率分布，由softmax函数产生的结果其实是对于数据的预测分布，预测分布和真实分布差值叫做KL散度或者是相对熵。我们希望的是预测值尽量靠近真实分布，也就是希望相对熵可以越来越小。**相对熵又等于交叉熵减去数据真实分布的熵，后者是确定的，所以最小化相对熵就等价于最小化交叉熵**。这就是交叉熵损失函数的由来，衡量的是预测值和真实标签之间的差异性，训练的目的是不断减少Loss，也就是让预测值不断靠近真实值。
>
> 相对熵：KL散度，用于衡量两个概率分布差异

#### Q：relu相比sigmod在防止过拟合上的优势

> relu函数较为简单，sigmod求导更为复杂，sigmod在两边梯度变化越来越小，逐渐变为0，容易陷入局部最小值。深度学习中，每一层都进行了非线性变化，在层数有限的变化，变成非线性，当层数很多如resnet时，用sigmod相当于线被折了上千次，最后远看像曲线，relu当x=0时不可导，但能到达这个不可导的概率几乎为0，用relu可以方便求导，梯度都是constant，函数形式越简单。当函数拟合的较为简单，就能降低过拟合，sigmod计算量问题，不太支持层数非常多的情况，不方便调整计算过程。
>
> relu函数较为简单，拟合的函数更为简单，奥卡姆剃刀原理



##### **分类：**分类是监督学习的一个核心问题，在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification).
#### Q：常见的损失函数

1.0-1损失函数 （0-1 loss function） 
L(Y,f(X))={1,0,Y ≠ f(X)Y = f(X)
L(Y,f(X))={1,Y ≠ f(X)0,Y = f(X)
2.平方损失函数（quadratic loss function) 
L(Y,f(X))=(Y−f(x))2
L(Y,f(X))=(Y−f(x))2
3.绝对值损失函数(absolute loss function) 
L(Y,f(x))=|Y−f(X)|
L(Y,f(x))=|Y−f(X)|
4.对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function) 

L(Y,P(Y|X))=−logP(Y|X)

####  Q：归一化，标准化

> 在机器学习中，归一化非常重要，可能导致模型坏掉或者训练出一个很奇怪的模型
>
> **线性函数归一化（Min-Max scaling）**：将原始数据归一化到【0，1】范围  
>
> ```python
> preprocessing.MinMaxScaler().fit_transform()
> ```
>
> $$X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}$$
>
> 实现等比例缩放，xnorm归一化后的数据
>
> **0均值标准化（Z-score standardization）**:归一化为均值为0，方差为1
>
> ```
> preprocessing.StandardScaler().fit_transform() # fit在计算传入数据的均值和方差，用这个均值和方差去计算transform传入的归一化后的结果
> ```
>
> $$z = \frac{x-mean}{std}$$
>
> 以上为两种比较普通但是常用的归一化技术，那这两种归一化的应用场景是怎么样的呢？什么时候第一种方法比较好、什么时候第二种方法比较好呢？下面做一个简要的分析概括：
> 1、在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。
>
> 2、在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。



### Q: bias variance

> bias 和variance 分别从两个方面描述了学习到的模型和真实模型之间的差异，这个真实模型是指获得所有可能的数据集合并且在这个数据集合上Loss最小化
>
> bias是用所有可能的训练数据集训练出来的所有模型的输出的平均值与真实模型的输出值之间的差异
>
> variance是不同的数据集训练出来的模型的输出值之间的差异
>
> 假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习，不包括最近邻等方法）。在训练过程的最初，bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大差异。
>
> 而随着训练过程的进行，bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得时间太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了我们的模型在很多其他的数据集上就无法获得很好的效果，也就是所谓的overfitting（过学习）。

### Q：交叉验证

> 留一交叉验证：每次从个数为N的样本集中，取出一个样本作为验证集，剩下的N-1个作为训练集，重复进行N次。最后平均N个结果作为泛化误差估计。基于数据完全切分，重复次数多，计算量大，因为折数就是样本的个数，有几个样本做几次循环，测试集就一个实例样本了
>
> K折交叉验证：把数据分成k份，每次拿一份作为测试集，剩下k-1份作为训练集，重复k次，最后平均k次的结果，作为误差评估的结果。









